RAG êµ¬í˜„í•˜ê¸° ì „ ì½ì—ˆì„ ë• ì™€ë‹¿ì§€ ì•Šë˜ ë…¼ë¬¸ì¸ë°ìš”. êµ¬í˜„í•œ ë’¤ ì½ì–´ë³´ë‹ˆ ë” ë§Žì€ ì¸ì‚¬ì´íŠ¸ê°€ ë˜ì–´ ê¸°ë¡í•©ë‹ˆë‹¤. ë…¼ë¬¸ ë‚´ìš©ì€ ì˜ì–´ë¡œ, ì²¨ì–¸ì€ í•œêµ­ì–´ë„ í•¨ê»˜ ì ì—ˆìŠµë‹ˆë‹¤.

## ðŸ“„ Retrieval-Augmented Generation for Large Language Models: A Survey
> Yunfan Gao et al.
> 
> 27 Mar 2024

### Background
**1. Rise of the Transformer architecture**
   - The Transformer architecture focused on enhancing LMs by incorporating additional knowledge through Pre-Training Models, aiming refining pre-training techs.
     
**2. ChatGPT**
  - `ICL` (In-Context Learning)
    
**3. RAG's shift**
  - There had been RAG techs, but researches shifted their way:
      - (previous) providing better info. for LLMs
      - â–¶ï¸ answer more complex and knowledge-intensive tasks
  - Basic structure of RAG: `Retrieval`, `Generation`, `Augmentation`

### RAG vs. FT
> _The choice between RAG and FT depends on the specific needs for data dynamics, customization, and computational capabilities in the application context._
![image](https://github.com/yuneun92/personal_study/assets/101092482/fb82dc65-6c22-463c-af9c-10ecce4fdfd3)

- `Prompt Engineering`: leverages a model's inherent capabilities with minimum necessity for external knowledge and model adaptation
- `RAG`: ideal for precise information retrieval tasks.
  - ðŸ‘ effetive utilization of external knowledge sources with high interpretability.
  - ðŸ‘Ž higher latency
- `FT`: suitable for scenarios requiring replication of specific structures, styles, or formets
  - ðŸ‘ deep customization of the model's behavior and styles.
    - reduce hallucination
  - ðŸ‘Ž more static, requiring retraining for updates
    - demands significant computer resources
    - may face challenges with unfamilier data.
    - LLMs struggle to learn new factual information through unsupervised fine-tuning.
    


### Overview of RAG

#### [TYPES]

1. `Naive RAG` : `"Retrieve-Read generation"` (Indexing â†’ Retrieval â†’ Generation)
   - `Indexing` : cleaning & extracting of raw data â†’ chunking â†’ encoding text and store vectors in vector database
   - `Retrieval` : transform a query into a vector repr â†’ compute similarity score â†’ priotize and retrieve the top K chunks
   - `Generation` : synthesize the query & selected chunks into a coherent prompt â†’ LLM formulate a response
   - _**`Drawbacks`**_
     - `Retrieval Challenges` : precision, recall ðŸ‘Ž â–¶ï¸ misaligned / irrelevant chunks, missing crucial info.
       - ìœ ì‚¬ë„ ì ìˆ˜ì—ë§Œ ì˜ì¡´í•´ ì¢‹ì€ ê²€ìƒ‰ ì„±ëŠ¥ì„ ê°–ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥ì— ê°€ê¹ìŠµë‹ˆë‹¤.
       - ì €ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ë¡œ ì›Œë“œ ìž„ë² ë”© + LLM ìž„ë² ë”©ì„ ì‚¬ìš©í–ˆëŠ”ë°ìš”, ì´ ì—­ì‹œ ì •í™•í•œ ë‹¨ì–´ì˜ ì¼ì¹˜ë¥¼ ë³´ëŠ” ê²ƒì€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— bm25ê¹Œì§€ ê°™ì´ ì‚¬ìš©í• ê¹Œ ìƒê° ì¤‘ìž…ë‹ˆë‹¤.
     - `Generation Difficulties` : hallucination
       - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ë§Œìœ¼ë¡œ í•œê³„ê°€ ìžˆë‹¤ê³  ëŠê»´ì„œ, p-tuning â†’ PEFT â†’ Fine Tuning ìˆœìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•´ë³¼ ìƒê°ìž…ë‹ˆë‹¤. 
     - `Augment Hurdles` : disjointed or incoherent outputs, redundancy
       
     ì´ ì•½ì ë“¤ì„ ì™„ì „ížˆ í•´ì†Œí•  ìˆ˜ ìžˆëŠ” RAG ê¸°ë²•ì´ ìžˆì„ì§€ëŠ” ëª¨ë¥´ê² ì§€ë§Œ.. Advanced RAGì—ì„œ í›¨ì”¬ ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ìž…ë‹ˆë‹¤.
2. `Advanced RAG` : _Enhance retrieval quality_ ; pre-retrieval and porst-retrieval stretegies
   - `Pre-retrieval process` : oprimize the indexing structure and the original query
     - enhance data granularity(ì„¸ë¶„í™”)
     - optimize index structure
     - add metadata
   - `Post-retrieval process` : rerank chunks, context compressing
     - Re-ranking: LlamaIndex, LangChain, HayStack
     - Select essential information: emphasize critical sections, shortening the cotext to be processed
       - ë°ì´í„°ì˜ í’ˆì§ˆì„ ì €í•´í•˜ëŠ” í† í°ì´ ë§Žì„ ë•Œ, LLMì€ ë‹µë³€ì„ ìž˜ ìƒì„±í•´ë‚´ì§€ ëª»í•©ë‹ˆë‹¤. 
       
3. `Modular RAG`
   - ðŸ’¡ **Search Module** :
     1. íŠ¹ì • ì‹œë‚˜ë¦¬ì˜¤ì— ì ì‘ - LLM ìƒì„± ì½”ë“œ, ì¿¼ë¦¬ ì–¸ì–´ ë“±.
     2. ë©€í‹° ì¿¼ë¦¬  - parellel vector search, re-ranking (ë‹¨ì–´ + ì˜ë¯¸ ëª¨ë‘ í¬ì°©í•  ìˆ˜ ìžˆë„ë¡)
   - ðŸ’¡ **Memory Module** :
     1. LLMì—ê²Œ ì£¼ëŠ” ë°ì´í„°ê°€ ë°˜ë³µì ì¸ ìžê¸° ë³´ì™„ ê³¼ì •ì—ì„œë„ ê¸°ì¡´ì˜ ë°ì´í„° ë¶„í¬ì™€ ìœ ì‚¬í•˜ë„ë¡ í•¨.
     2. ìš”ì•½, íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ì—ì„œì˜ ê²€ìƒ‰, ë‹¤ë¥¸ ì •ë³´ ìŠ¤íŠ¸ë¦¼ì„ í•©ì¹˜ëŠ” ê²ƒ ë“±
   - ðŸ’¡ **Predict Module**
   - ðŸ’¡ **Task Adapter Module**

---

#### [STEPS]

1. `RETRIEVAL`
   - **Source**
     - `Data structure`
      1. semi-structured data : PDF ... (text + table)

         ì²˜ë¦¬í•˜ê¸° ê°€ìž¥ ì–´ë ¤ìš´ ë°ì´í„° íƒ€ìž…..
         - Text-2-SQL queries: TableGPT
         - transform tables into text format
      2. structured data : knowledge graph (KG)
         - KnowledGPT : generates KB search queries and stores knowledge in a bersonalized base
         - G-Retriever : GNNs + LLMs + RAG // Prize-Collecting Steiner Tree (PCST) optimization prob. for targeted graph retrieval.
      3. unstructured data : text
      4. LLMs-Generated Content
         - SKR: classifies questions as `known` or `unknown`, applying retrieval enhancement selectively.
         - GenRead: replaces the retriever with an LLM genenrator ; better alignment with the pre-training objectives of causal lang. modeling.
         - Selfmem: iteratively creates an unbounded memory pool with a retrieval-enhanced generator
      - `Retrieval Granularity` : ì²­í¬ë¥¼ ë„ˆë¬´ ìž˜ê²Œ ìª¼ê°œë©´ ë¦¬íŠ¸ë¦¬ë²„ê°€ ì œëŒ€ë¡œ ê¸°ëŠ¥í•˜ê¸° ë” ì–´ë µê³ , ì²­í¬ í¬ê¸°ê°€ ë„ˆë¬´ í¬ë©´ ìž„ë² ë”©ì´ ì ì ˆížˆ ì˜ë¯¸ë¥¼ ë‚´í¬í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
        1. í† í°, êµ¬, ë¬¸ìž¥, `ëª…ì œ`, ì²­í¬, ë¬¸ì„œ ...
           > **Proposition(ëª…ì œ)**: atomic expressions in the text, each encapsuating a unique factual segment and presented in a concise, self-contained natural language format.
        2. KG : Entity, Triplet, sub-Graph
     
2. `GENERATION`
   - 
3. `AUGMENTATION`
   -
   
---

## ðŸ“„ A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models
> Wenqi Fan et al.
>
> 17 Jun 2024
>
> https://arxiv.org/pdf/2405.06211

![image](https://github.com/yuneun92/personal_study/assets/101092482/3c7b49d5-42f6-4549-8aba-cb6328ac8a05)

