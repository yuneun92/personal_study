RAG 구현하기 전 읽었을 땐 와닿지 않던 논문인데요. 구현한 뒤 읽어보니 더 많은 인사이트가 되어 기록합니다. 논문 내용은 영어로, 첨언은 한국어도 함께 적었습니다.

This is the review of 

## Retrieval-Augmented Generation for Large Language Models: A Survey
> Yunfan Gao et al.
> 27 Mar 2024

### Background
1. Rise of the Transformer architecture
   - The Transformer architecture focused on enhancing LMs by incorporating additional knowledge through Pre-Training Models, aiming refining pre-training techs.
2. ChatGPT
  - `ICL` (In-Context Learning)
3. RAG's shift
  - There had been RAG techs, but researches shifted their way:
      - (previous) providing better info. for LLMs
      - ▶️ answer more complex and knowledge-intensive tasks
  - Basic structure of RAG: `Retrieval`, `Generation`, `Augmentation`

### Overview of RAG

[TYPES]

1. Naive RAG
2. Advanced RAG
3. Modular RAG

[STEPS]

1. RETRIEVAL
2. GENERATION
3. AUGMENTATION
