RAG Íµ¨ÌòÑÌïòÍ∏∞ Ï†Ñ ÏùΩÏóàÏùÑ Îïê ÏôÄÎãøÏßÄ ÏïäÎçò ÎÖºÎ¨∏Ïù∏Îç∞Ïöî. Íµ¨ÌòÑÌïú Îí§ ÏùΩÏñ¥Î≥¥Îãà Îçî ÎßéÏùÄ Ïù∏ÏÇ¨Ïù¥Ìä∏Í∞Ä ÎêòÏñ¥ Í∏∞Î°ùÌï©ÎãàÎã§. ÎÖºÎ¨∏ ÎÇ¥Ïö©ÏùÄ ÏòÅÏñ¥Î°ú, Ï≤®Ïñ∏ÏùÄ ÌïúÍµ≠Ïñ¥ÎèÑ Ìï®Íªò Ï†ÅÏóàÏäµÎãàÎã§.

This is the review of 

## üìÑ Retrieval-Augmented Generation for Large Language Models: A Survey
> Yunfan Gao et al.
> 
> 27 Mar 2024

### Background
1. Rise of the Transformer architecture
   - The Transformer architecture focused on enhancing LMs by incorporating additional knowledge through Pre-Training Models, aiming refining pre-training techs.
2. ChatGPT
  - `ICL` (In-Context Learning)
3. RAG's shift
  - There had been RAG techs, but researches shifted their way:
      - (previous) providing better info. for LLMs
      - ‚ñ∂Ô∏è answer more complex and knowledge-intensive tasks
  - Basic structure of RAG: `Retrieval`, `Generation`, `Augmentation`

### Overview of RAG

#### [TYPES]

1. `Naive RAG` : `"Retrieve-Read generation"` (Indexing ‚Üí Retrieval ‚Üí Generation)
   - 
2. Advanced RAG
   - 
3. Modular RAG
   -
---

#### [STEPS]

1. RETRIEVAL
2. GENERATION
3. AUGMENTATION
