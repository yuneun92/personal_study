import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import json, torch, os, re
import streamlit as st
from setproctitle import setproctitle
setproctitle("streamlit_openai")

from deep_translator import GoogleTranslator
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.tools import tool

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings

import streamlit as st
import time


def clear_cache():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

embedding_model = HuggingFaceEmbeddings(
                model_name='jhgan/ko-sroberta-multitask',
                model_kwargs={'device': "cuda:0"},
                encode_kwargs={'normalize_embeddings' : True}
            )


# í¬ë¡œë§ˆ DB ì ‘ì†, ë„íë¨¼íŠ¸ì˜ ì»¨í…ì¸ ì™€ ë©”íƒ€ ì •ë³´ ë©”ëª¨ë¦¬ì— ë¶ˆëŸ¬ì˜¤ê¸°
chroma_host = 'í˜¸ìŠ¤íŠ¸ë¥¼ ì…ë ¥'
chroma_port = '8000'
chroma_client = chromadb.HttpClient(host=chroma_host, port=chroma_port)    
collection_name = "ai_nlp_langchain_kr_v1.3"

# @st.cache_data(show_spinner=False, allow_output_mutation=True, ttl=3600)
def translate(corpus):
    rst = GoogleTranslator(source='auto', target='ko').translate(text=corpus) 
    return rst
    
# @st.cache_data(show_spinner=True, ttl=3600, hash_funcs={chromadb.HttpClient: id})    
def get_collection_documents():
    collection = chroma_client.get_collection(collection_name)
    data = collection.get()
    documents = data['documents']
    metadatas = data['metadatas']
    return documents, metadatas

class Document_create:
    def __init__(self, page_content, metadata):
        self.page_content = page_content
        self.metadata = metadata

def trans_data():
    documents, metadatas = get_collection_documents()
    
    output_documents_representation = [{
        'page_content': doc,
        'metadata': {'doc': meta['doc'], 'page': meta['page'], 'origin_cont': meta['origin_cont']}
    } for doc, meta in zip(documents, metadatas)]
    
    docs = [Document_create(page_content=doc['page_content'], metadata=doc['metadata']) for doc in output_documents_representation]

    return docs

# ë¶ˆëŸ¬ì˜¨ ë©”ëª¨ë¦¬ì—ì„œ ê²€ìƒ‰í•˜ê¸°
@tool('retriever')
def retriever(query):
    weights = [0.6, 0.4]

    chroma_docs = trans_data()
    
    bm25_retriever = BM25Retriever.from_documents(chroma_docs)
    bm25_retriever.k = 3
    
    chroma_retriever = Chroma.from_documents(chroma_docs, embedding_model)
    chroma_retriever = chroma_retriever.as_retriever(search_kwargs={'k':3})

    # initialize the ensemble retriever
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, chroma_retriever], weights=weights
    )

    response = ensemble_retriever.invoke(query)
    docs = [response[0].page_content, response[1].page_content, response[2].page_content]
    meta = [response[0].metadata, response[1].metadata, response[2].metadata]
    return docs, meta

tools = [retriever]

@st.cache_resource
def load_model():
    return ChatOpenAI(model_name="gpt-3.5-turbo", openai_api_key='openai_api_key', streaming=True)
    
llm = load_model()
memory_key = "history" #ëŒ€í™” ë‚´ì—­ì„ ê¸°ë¡í•  ë©”ëª¨ë¦¬ í‚¤ì…ë‹ˆë‹¤.

from langchain.agents.openai_functions_agent.agent_token_buffer_memory import (
    AgentTokenBufferMemory,
)

memory = AgentTokenBufferMemory(memory_key=memory_key, llm=llm)

from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent
from langchain.schema.messages import SystemMessage
from langchain.prompts import MessagesPlaceholder

#chat gptì—ê²Œ ì¤„ promptì…ë‹ˆë‹¤
system_message = SystemMessage(
    content=(
        "You are a nice customer service agent."
        "Do your best to answer the questions. "
        "Feel free to use any tools available to look up "
        "relevant information, only if necessary"
        "If you don't know the answer, just say you don't know. Don't try to make up an answer."
        "Make sure to answer in Korean"
        "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”"
        "ì¤„ê¸€ë¡œ ì„¤ëª…í•˜ì„¸ìš”"
    )
)

prompt = OpenAIFunctionsAgent.create_prompt(
    system_message=system_message,
    extra_prompt_messages=[MessagesPlaceholder(variable_name=memory_key)],
)

# í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ agentë¥¼ ìƒì„± ë° ì‹¤í–‰í•©ë‹ˆë‹¤
agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)

from langchain.agents import AgentExecutor

agent_executor = AgentExecutor(
    agent=agent,
    tools=tools, # ì—¬ê¸°ì„œì˜ toolì€ ë°ì½”ë ˆì´í„°ë¡œ ì§€ì •í•˜ê³  tools ë¦¬ìŠ¤íŠ¸ë¡œ ë” ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    memory=memory,
    verbose=True,
    return_intermediate_steps=True,
)

####ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ ê´€ë ¨ í•¨ìˆ˜ ####
def load_chat_history():
    '''
    ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì´ ìˆëŠ” ê²½ìš° time, prompt, responseë¥¼ ìš”ì†Œë¡œ í•˜ëŠ” íŠœí”Œ ë¦¬ìŠ¤íŠ¸, 
    ê¸°ë¡ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    '''
    try:
        with open("chat_history.json", "r") as file:
            chat_history = json.load(file)
    except FileNotFoundError:
        return []

    return chat_history

def save_chat_history(chat_history):
    '''ëŒ€í™” ê¸°ë¡ì„ JSON íŒŒì¼ì— ì €ì¥'''
    with open("chat_history.json", "w") as file:
        json.dump(chat_history, file)


def process_message(prompt, response):
    '''ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸ ë° ì €ì¥'''
    if not st.session_state.messages:
        chat_history.append((datetime.now().strftime("%Y-%m-%d %H:%M:%S"), prompt, response))
        save_chat_history(chat_history)


def main():
    st.header('ğŸ’¬ ì‚¬ë‚´ ì±—ë´‡')
    st.sidebar.title("Chat History")
    user_messages = [message['content'] for message in st.session_state.get("messages", []) if message["role"] == "user"]

    for idx, msg in enumerate(user_messages, start=1):
        st.sidebar.markdown(f"**ì§ˆë¬¸ {idx}.** {msg}")
        
    llm = load_model()
    
    chat_history = load_chat_history()

        
    if st.sidebar.button("ìƒˆë¡œìš´ ì±„íŒ… ì‹œì‘"):
        st.session_state.messages = []   
        
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}] #ê¸°ë³¸ ë©”ì‹œì§€
    
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message['content'])
    
    if prompt := st.chat_input():
        # ì‚¬ìš©ì ì¸í’‹ì„ prompt ë³€ìˆ˜ë¡œ ë°›ìŠµë‹ˆë‹¤
        st.session_state.messages.append({"role": "user", "content": prompt})
        # session stateì˜ ë©”ì‹œì§€ì—, roleê³¼ contentë¡œ êµ¬ì„±ëœ dictionaryë¡œ ë°›ì•„ì˜¨ ìœ ì € í”„ë¡¬í”„íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        with st.chat_message("user"):
            st.markdown(prompt)
        prompt = translate(prompt)
        # í”„ë¡¬í”„íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë°”ê¿” ë¦¬íŠ¸ë¦¬ë²„ ì‹œìŠ¤í…œìœ¼ë¡œ ë„˜ê¹ë‹ˆë‹¤
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            retrived_output = retriever(translate(prompt))  
            meta = retrived_output[1]
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°›ì•„ì˜µë‹ˆë‹¤ (ë¬¸ì„œ, ë©”íƒ€ ë°ì´í„°ë¡œ êµ¬ì„±ëœ íŠœí”Œë¡œ, ë¬¸ì„œëŠ” ë¬¸ìì—´ì´ë©° ë©”íƒ€ë°ì´í„°ëŠ” ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤. 
            # í‚¤ê°’ì€ doc, page, origin_contë¡œ docì€ ë¬¸ì„œì˜ ì œëª©(ë¬¸ìì—´), pageëŠ” ì²­í¬ì˜ ë¬¸ì„œì—ì„œì˜ í˜ì´ì§€(ì •ìˆ˜), origin_docì€ ì›ë¬¸ ë¬¸ì„œ ë‚´ìš©(ë¬¸ìì—´)ì…ë‹ˆë‹¤)           
            refer = f"ì•„ë˜ëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤. \n\n1. '{meta[0]['doc']}'ì˜ {meta[0]['page']}í˜ì´ì§€ ì…ë‹ˆë‹¤.\n\n2. '{meta[1]['doc']}'ì˜ {meta[1]['page']}í˜ì´ì§€ ì…ë‹ˆë‹¤. \n\n3. '{meta[2]['doc']}'ì˜ {meta[2]['page']}í˜ì´ì§€ ì…ë‹ˆë‹¤.\n\n"
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë™ê¸°ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤
            for chunk in (refer).split(' '):
                full_response += chunk + " "
                time.sleep(0.05)
                message_placeholder.markdown(full_response + "â–Œ")
            # gpt apiì˜ ì¶”ë¡  ê²°ê³¼ë¥¼ ë°›ì•„ì™€ ë™ê¸°ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤    
            result = agent_executor({"input": prompt})
            for chunk in (result['output']).split(' '):
                full_response += chunk + " "
                time.sleep(0.05)
                message_placeholder.markdown(full_response + "â–Œ")
                
            message_placeholder.markdown(full_response)
        st.session_state.messages.append({"role": "assistant", "content": full_response}) # ì±—ë´‡ì˜ ëŒ€ë‹µë„ session stateì˜ ë©”ì‹œì§€ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤


if __name__ == "__main__":
    clear_cache()
    main()


